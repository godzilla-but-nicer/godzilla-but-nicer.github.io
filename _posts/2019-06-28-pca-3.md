---
layout: post
title:  "Whats the deal with PCA: Doing the thing"
date:   2019-06-28 00:00:00 -0500
categories: methods
---

## 4. Sort Eigenvectors and Eigenvalues

Now that we have found the eigenvectors of our covariance matrix–our principle
components–its time to do something with them. The scalar eigenvalues associated
with each of the eigenvectors tell us in relative terms how much variance is
captured in the direction represented by each eigenvector.

We can think of variance as potential information. If we have some feature (or
component) that doesn’t vary it all, then we don’t learn anything new by looking
at it. It has no ability to influence how we classify our samples. Conversely,
high variance indicates a high amount of information contained in that feature
or component. There is information there because we don’t know what the value
will be for a particular sample. Finding out the value tells us something
meaningful about the sample.

We want to know which of our principal components contain the most information
and we will do this with a metric called the variance explained ratio.

$$ VER_j = \frac{\lambda_j}{\sum_{j=1}^d \lambda_j} $$

Here, $\lambda_j$ is the $j$th eigenvalue and $d$ is the number of dimensions in
our data.

We know that all of our features taken together in our dataset contain all of
the variance. Therefore the VER gives us the fraction of total variance
explained by each principal component So far we have not removed anything, we’ve
simply found new directions. We have the same number of new directions as we had
features so all of the variance must still be contained in them.

For this final section, we will be working with all four of the features in the
iris dataset. Shown below is some python code that will do these calculations
and generate a plot where we can easily examine our principal components.

{% highlight python %}
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.preprocessing import StandardScaler


# load iris
iris = datasets.load_iris()
X = iris.data

# Standardize
sc = StandardScaler()
X_std = sc.fit_transform(X)

# get covariance matrix
X_cov = np.cov(X_std.T, ddof=0)

# extract eigenvalues and eigenvectors
eigenvals, eigenvecs = np.linalg.eig(X_cov)

# find variance explained ratios
tot = np.sum(eigenvals)
ver = [(i / tot) for i in sorted(eigenvals, reverse=True)]

# get cumulative variance explained ratio
cum_ver = np.cumsum(ver)

# plot it
plt.figure(figsize=(8, 6))
plt.bar(range(1, len(eigenvals) + 1), ver, align='center',
        label='individual variance explained', alpha=0.5)
plt.step(range(1, len(eigenvals) + 1), cum_ver, where='mid',
        label='cumulative variance explained')
plt.xlabel('Principal component index')
plt.ylabel('Explained variance ratio')
plt.legend(loc='best')
plt.show()
{% endhighlight %}

![explained variance ratio](/assets/pca-3/ver.png)

What the plot above tells us is that our first principal component contains
about 70% of the variance, our second principal component contains about 20% of
the variance, the third contains almost all of the remaining variance and the
forth has only a negligible amount. The line above shows the cumulative variance
captured by the principal components.

## 5. Keep some of the components

Sometimes we might have hard constraints on the number of dimensions we need.
Maybe we want to make some kind of visualization and need exactly 2 dimensions.
Maybe we want to make a confusing visualization and need exactly 3. Maybe we
want to optimize a classifier that struggles with high dimensionality and we
just want as few as possible. Maybe we simply need our dataset to be smaller.

Regardless, there are some lessons to be learned from the plot shown above. What
we aim to do with PCA is represent as much of the variance as we can with fewer
dimensions than we started with. That fourth principal component is largely,
maybe completely, useless. The variance is so small that it is almost certainly
just noise. In fact, the third component is not very useful either. We are able
to represent over 90% of the variance using only the first two dimensions, so
let’s keep only those two.

## 6. Transform the dataset

Now that we know our principal components and which of them we want to use, we
need to actually use them. Specifically, we need to project our data onto them.
Once again, we will turn to linear algebra to make this easy for us. We will
start transforming our data by making a matrix that will act as a linear
transformation to project our data onto the first two principal components.

This is actually very easy to do. All we need to do is arrange the eigenvectors
that represent the principal components we want to use as the columns of a
matrix. This will give us the projection matrix, $W$.

$$ W = \begin{bmatrix} 0.52 & -0.37 \\ -0.27 & -0.92 \\ 0.58 & -0.02 \\ 0.56 & -0.07 \end{bmatrix} $$

$W$ is a $4 \times 2$ matrix. In general our matrix will always be a $d \times
k$ matrix where $d$ is the number of dimensions in the original data and $k$ is
the number of dimensions we want out.

We can now use our new matrix to transform our dataset. Remember that our data
is stored as an $n \times d$ matrix, $X$, where $n$ is the number of samples and
$d$ is the number of features. We can simply multiply by to get an matrix back.
in this case, one that consists of the original data projected onto our selected
principal components.

$$ XW = X' $$

In the above equation, $X'$ is our new, transformed dataset. Anything we might
have done to the original dataset we can do to this one. $X'$ retains most of
the information (>90% as we saw in our bar plot) contained in the original
dataset but has been compressed by half.

Now let’s look at how we can do this stuff in python:

{% highlight python %}
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.preprocessing import StandardScaler

# load iris
iris = datasets.load_iris()
X = iris.data

# Standardize
sc = StandardScaler()
X_std = sc.fit_transform(X)

# get covariance matrix
X_cov = np.cov(X_std.T, ddof=0)

# extract eigenvalues and eigenvectors
eigenvals, eigenvecs = np.linalg.eig(X_cov)


# make tuples consisting of eigenvalues and eigenvectors
# and sort them by eigenvalue
eigenpairs = [(np.abs(eigenvals[i]), eigenvecs[:, i]) 
              for i in range(len(eigenvals))]
sorted_epairs = sorted(eigenpairs, key=lambda x: x[0], 
                       reverse=True)

# select how many components to keep
k_keep = 2
d_orig = X.shape[1]

# set up empty transformation array
W = np.zeros((d_orig, k_keep))

# fill columns of array with components
for col in range(k_keep):
    W[:, col] = sorted_epairs[col][1]

print('Transformation Matrix:\n', W)
{% endhighlight %}

```text
> Transformation Matrix:
   [[ 0.52106591 -0.37741762]
   [-0.26934744 -0.92329566]
   [ 0.5804131  -0.02449161]
   [ 0.56485654 -0.06694199]]
```

{% highlight python %}
X_pca = X_std.dot(W)
print('Before:\n', X_std[:3], '\n')
print('After:\n', X_pca[:3])
{% endhighlight %}

```text
> Before:
   [[-0.90068117  1.01900435 -1.34022653 -1.3154443 ]
   [-1.14301691 -0.13197948 -1.34022653 -1.3154443 ]
   [-1.38535265  0.32841405 -1.39706395 -1.3154443 ]] 
  
  After:
   [[-2.26470281 -0.4800266 ]
   [-2.08096115  0.67413356]
   [-2.36422905  0.34190802]]
```

{% highlight python %}
# let's indicate which irises are which for this plot
# we will need the actual target classes
y = iris.target
label = iris.target_names

# make some nice shapes for our different species
shapes = ['o', 's', '^']

plt.figure(figsize=(8, 6))
for species in np.unique(y):
    X_species = X_pca[y == species, :]
    plt.scatter(X_species[:, 0], X_species[:, 1],
                marker=shapes[species], label=label[species])
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend(loc='best')
plt.show()
{% endhighlight %}

![first two principal components of iris data](/assets/pca-3/pca.png)

Above we can see the first two principal components of the Iris dataset. We see
that in this reduced dimensionality space we see a lot of species separation
along the first principal component. This is the kind of thing we hope for when
we do PCA. Its also consistant with our understanding of the first principal
componant capturing the greatest variance.

Of course, we can also do PCA without invoking linear algebra functions using
`sklearn.preprocessing.PCA` which makes it all quite easy.