---
layout: post
title:  "Running Python on Indiana University HPC"
date:   2020-03-05 00:00:00 -0500
categories: tools
---

## This guide is mostly still right but scratch is now slate and more machines are available

This guide will show you how to run python scripts on the High Performance
Computing (HPC) resources at IU. In order to use these resources you have to be
affiliated with the university (obviously) and specifically, these machines are
mostly restricted to graduate students, staff, and faculty. We will cover the
available machines, logging in and uploading scripts, using modules and job
scripts for the Slurm workload manager on Big Red 3, and considerations for your
python script when running it on HPC.

## High Performance Computers at IU

- *Big Red 3*: This is the big general purpose supercomputer. I have the most
  experience using this machine. It has the most powerful CPUs of any of these
  machines but no GPUs. It has limited memory compared to Carbonate.

- *Carbonate*: This machine is designed for high-throughput and data intensive
  jobs. It has four times the memory per node (8x if you use “large memory
  nodes”) than Big Red 3 and some of the nodes are equipped with GPUs. This is
  the right choice if you are doing something like training a neural network.

- *Karst*: This is another more general supercomputer. It is less powerful than
  Big Red 3. There may be reasons to use this machine but I do not know what
  they are.

## Storing your data

You can keep data on one of the HPC machines. This is more-or-less permanent and
each user is allocated 100 GB in their home directory. These home directories
are created when you create an account on one of these machines. The reading and
writing speed for this storage is not designed for high-throughput applications.

For long term permanent storage you can use the scholarly data archive (SDA).
Each user with an SDA account is allocated 50 TB of storage. The data is at
least potentially stored on actual tape (!?) and accessing it is slow if you
don’t use a special API for accessing it. This is not a good choice for data you
want to access during execution.

If you need to read and write a lot of data during the execution of your script,
you will want to use the Data Capacitor II (dc2) scratch storage. You can access
this from the HPC machine you are logged into at (where username is your IU
username):

```text
/N/dc2/scratch/username
```

This storage is designed for fast I/O but data not accessed for 60 days may be
deleted.

## Creating your account on HPC

Before you can use any of these machines you will need to first create an
account on it. In order to do so, first log into [HPC
everywhere](http://hpceverywhere.iu.edu/). You should see a screen that looks
something like this:

![hpc everywhere, baby](/assets/iu-hpc/hpseverywhere-873x1024.png)

You can make new accounts from the access management page available in the
bottom left. Unlike mine (shown below), you won’t have the names of the HPC
machines listed in your computing accounts just yet. When you make a new account
they will ask you a few questions and make you check some boxes and so forth.

![access management, baby](/assets/iu-hpc/access-management-1024x804.png)

When you do this you will see the machine’s name appear in your Computing
Accounts list but this does not mean that your account is actually ready to use.
If you try to log in to the machine it will give you some kind of permission
denied message. It really does take some time for this to work. They say up to
24 hours on the website, in my experience its a lot less than that, more like a
few hours. Regardless, you will get an email once your account is actually ready
to use.

Do not put in a support ticket right after creating your account because you
can’t log in, they will get mad at you.

## Logging into HPC

With your account activated you can now log in to the machine, we will be
focusing on Big Red 3. If you are using Mac or Linux you do this through the
terminal, if you are on windows you will have to use a tool like
[PuTTY](https://www.chiark.greenend.org.uk/~sgtatham/putty/). We are going to
log in using `ssh`. Specifically, we will use the following command in the
terminal.

```bash
ssh username@bigred3.uits.iu.edu
```

You will be prompted to enter a password which is the same as your IU account
password and authenticate the login through Duo.

![duo is cool and people should say nice things about them](/assets/iu-hpc/duo_authentication.png)

If you are using PuTTY this process will look different, you will still be
connecting through ssh but instead of using the terminal you will need to use
PuTTY’s GUI.

Once you’ve logged in, you interact with the remote machine using the terminal.
All of IU’s HPC machines, and pretty much all HPC machine in general, run Linux.
If you are not used to using the terminal this [blog
post](https://mrkaluzny.com/blog/terminal-101-getting-started-with-terminal/)
will help you get started.

## Putting your stuff on HPC

So now we can log in. Unless we want to write all of our scripts through Vim or
emacs directly on Big Red, we’re going to need to get our own files there. There
are two options for this `scp` and `sftp`. I usually use `sftp` so I’ll first
show how that works.

We can connect to Big Red 3 through `sftp` the same way we connect through `ssh`.

```bash
sftp username@bigred3.uits.iu.edu
```

Once we are logged in we can interact with the file system on Big Red with the
normal commands, `cd`, `ls`, `pwd` etc. We can also move around our local
machines file system with “local” versions of those commands by putting an ‘l’
in front like this: `lcd`, `lls`, `lpwd`. We can then put files from our local
machine on to Big Red and also get files from Big Red. In the following example
I will upload a file from my machine to Big Red and also download a file from
Big Red.

![sftp is a nice tool! it rocks!](/assets/iu-hpc/sftp.png)

Using `scp` we could accomplish the same thing with a couple of commands but would
be required to login for each one.

```bash
scp ~/Documents/demo/upload_me.py username@bigred3.uits.iu.edu:~/example
scp username@bigred3.uits.iu.edu:~/example/download_me.py ~/Documents/demo
```

If we want to move entire directories with either `scp` or `sftp`‘s `put` and
`get` commands we need to add `-r`. For example:

```bash
scp -r ~/Documents/demo username@bigred3.uits.iu.edu:~/example
```

```bash
put -r ~/Documents/demo
```

## Modules

Big Red 3 has a bunch of installed software so you will often not need to worry
about installing anything yourself. The software is bundled into modules. What
you will need to do is be aware of is what software you have loaded and what
software is available to load. When we are logged in to Big Red, we can get a
list of the modules we have loaded with `module list` and the available modules
with `module avail`. When I run `module list` This is what I see:

![so many modules!](/assets/iu-hpc/module-list.png)

What’s important to note that is that python is not anywhere in that list. We’ll
have to keep that in mind when we want to run our script. With `module avail` I
get a huge wall of text. What is important is that there are a couple of modules
for Python. One for Python 2 (`python/2.7.16`) and one for Python 3
(`python/3.6.9`). If we don’t have a python module loaded than Big Red won’t
understand the command `python` so we can’t run anything!

The Python modules also include every library I’ve ever tried to use although I
understand that it can’t possibly include every library. If you need to use a
library that isn’t in the available python modules you might have to reach out
to support (and potentially get bounced around a little bit to the machine
specific team). Another option is to use one of the `anaconda` modules
and install things yourself. I managed to figure out how to make this work but
I can't seem to replicate it.

## Jobs and Job Scripts

If everyone were constantly just running things on Big Red, it would be very
difficult to ensure everyone’s job actually got done. Big Red uses a system
called Slurm to handle jobs while Karst and Carbonate use an older system called
TORQUE. This matters only in that the syntax is different between the two
systems. The ideas are essentially equivalent.

We can start making our job script using the script generator on HPC everywhere.

![the magical script generator](/assets/iu-hpc/script_generator.png)

This is a really convenient way to get the header for your job script that tells
it some syntax-specific important things about your job. Each section in the GUI
is potentially important and they change the header in your script.

- Compute Resource: The machine you want to run your job on. This affects the
  syntax of the generated script.

- Queue: This affects how much of the machine we can request and how much time
  we can use. The options are general, grand, and debug. grand is reserved for
  IU grand challenge projects and you have to request access to use it. general
  allows us to request up to the entire machine for up to 7 days. debug limits
  us to 2 nodes and 4 hours. We may be able to move through the queue faster if
  we chose debug for small jobs.

- Walltime: This is how much time we want to reserve on the machine once our job
  starts. If we run out of walltime the job will just stop.

- Number of Nodes: Each node is essentially a computer. Our python scripts will
  not be capable of running on more than one node unless we do things that I
  have no idea how to do. The only software that will be running on more than
  one node is software specifically designed for HPC.

- Cores Per Node: While we probably can’t write our scripts to run on multiple
  nodes, multiple cores is feasible. However, unless our script is explicitly
  written to run in parallel, selecting more than one will not make it run
  faster.

- Memory Per Node: If we leave this at 0 it will default to 16GB. We can request
  more as needed.

- Job Title: This can be whatever we want. This name will be used in email
  notifications and in the log.

- Output Filename/Error Filename: This has nothing to do with the output of our
  Python code. This is a file that the HPC system writes to so if you put
  commands to give text updates this is where they will go. Errors will go to
  the same file by default.

 -Send email: If we put our email here then Big Red will email us when our job
 actually starts from the queue and/or when it aborts due to error or ends
 correctly.

 -Environmental Modules: This allows us to generate the commands to load or
 unload appropriate modules. The problem is that the tool on this page lists
 more modules than are actually available so I will do this part by hand.

- Job Commands: We can add our actual commands to run here but again I usually
  do this by hand in a different editor.

Typically I will use this to create my job script header then copy it to the
clipboard and finish it in `vim` or some other editor. If you’re running a
script for Python 3 that is stored in the same directory as the job script than
you can simply add two lines to this generated script.

```text
######  Module commands #####
module load python/3.6.9


######  Job commands go below this line #####
python3 my_script.py
```

Here is a complete (but slightly modified) job script for a job I ran recently.
I called the job script `demo_run.sh`.


```bash
#!/bin/bash
#####  Constructed by HPC everywhere #####
#SBATCH --mail-user=username@iu.edu
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --time=2-15:59:00
#SBATCH --partition=general
#SBATCH --mail-type=FAIL,BEGIN,END
#SBATCH --job-name=demo_job
#SBATCH --output=log

######  Module commands #####
module load python/3.6.9



######  Job commands go below this line #####
echo '###### move to script dir ######'
cd ~/demo_job/demo_dir
echo '###### Directory Changed! ######'

echo '###### Running evolve.py ######'
python3 evolve.py
echo '###### Run Complete! ######'
```

This script is requesting one core on one node for two days, 15 hours, and 59
minutes through the general queue. It is also requesting that I receive emails
whenever anything happens. It is writing output (those `echo` commands) and error
messages to a file called log which is the default name. I did not request a
specific amount of memory, 16 GB was fine for this job.

With this script it is very easy to add a job to the queue. We use the `sbatch`
command followed by the name of the job script.

```bash
sbatch demo_run.sh
```

This tells the Slurm job management system used by Big Red that we want to add
our job to the queue. Once our job is added we can check the status of the job
using the `squeue` command and specifying a user.

```bash
squeue -u username
```

It will take some time before our job is actually run but once it is in the
queue it is only a matter of time.

## Considerations for your python script

I mentioned earlier that there is no need to request more than one core if your
code is not written to run in parallel. I have no experience with
parallelization in Python but I found what seems to be a [very helpful
tutorial](https://www.machinelearningplus.com/python/parallel-processing-python/).
If this does work, and your code is parallelizable than there is no reason not
to request more than one core.

Another issue is that when you save output from python it actually goes into a
buffer. Even if you are saving output “as you go” by saving a file in each
iteration of a loop for instance, the files first just go to a buffer. The
buffer is flushed and actually written out only when the buffer is full or the
file is closed. The buffer size on a machine like Big Red is massive. If
something goes wrong with your job the buffer may not be flushed. What this
means is that you might have a job run for two days, 15 hours, and 59 minutes
only to run out of reserved time and stop without writing any of the output
files.

A workaround for this is to manually
[flush](https://www.tutorialspoint.com/python/file_flush.htm) the buffer of the
file.

{% highlight python %}
import numpy as np

# make some array
arr = np.array([1, 2, 3])

# open a file to store the array in
fout = open('my_arr.npy', 'wb')
np.save(fout, arr)

# force writing out the changes
fout.flush()
{% endhighlight %}

This should work with any kind of file you might want to save but it is still
probably best to use caution and test things before you submit a huge job.
